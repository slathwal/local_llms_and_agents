{
  "hash": "6d23ea468ac1bc0e8e00dc79c9f02296",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Using chat models and messages in Graphs - LangGraph\"\nauthor: \"Shefali Lathwal\"\ndate: \"2025-04-25\"\ndate-modified: last-modified\neditor: visual\ntoc: true\nformat: html\njupyter: python_langchain\nexecute:\n  freeze: auto\n---\n\n\n# Goals\n\n-   We will use chat messages in our graph\n-   We will use chat models in our graph\n\n# Messages\n\nChat models can use `messages`, which capture different roles within a conversation.\n\nLangChain supports various message types, including `HumanMessage` , `AIMessage`, `SystemMessage`, and `ToolMessage`.\n\nThese represent messages from the user, from the chat model, for the chat model to instruct behaviour, and from a tool call.\n\nEach message can be supplied with the following:\n\n-   `content` - content of the message\n\n-   `name` - optional, who is creating the message\n\n-   `response_metadata` - optional, a dictionary of metadata that is often specific to each model provider\n\n::: {#da2376f5 .cell execution_count=1}\n``` {.python .cell-code}\nfrom pprint import pprint\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\nmessages = [AIMessage(content = f\"So you said you were researching ocean mammals?\", name = \"Model\")]\nmessages.extend([HumanMessage(content=f\"Yes, that's right.\", name = \"Lance\")])\nmessages.extend([AIMessage(content = f\"Great, what would you like to learn about?\", name = \"Model\")])\nmessages.extend([HumanMessage(content = f\"I want to learn about the best place to see Orcas in the US. Keep your answer to one paragraph.\", name = \"Lance\")])\n\nfor m in messages:\n    m.pretty_print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n================================== Ai Message ==================================\nName: Model\n\nSo you said you were researching ocean mammals?\n================================ Human Message =================================\nName: Lance\n\nYes, that's right.\n================================== Ai Message ==================================\nName: Model\n\nGreat, what would you like to learn about?\n================================ Human Message =================================\nName: Lance\n\nI want to learn about the best place to see Orcas in the US. Keep your answer to one paragraph.\n```\n:::\n:::\n\n\n# Chat Models\n\nChat Models can use a sequence of messages as input and support message roles as discussed below. We will use a local model from Ollama\n\n::: {#49adc908 .cell execution_count=2}\n``` {.python .cell-code}\nfrom langchain_ollama import ChatOllama\n\ngemma3 = ChatOllama(model = \"gemma3:4b\", temperature = 0)\nresult = gemma3.invoke(messages)\ntype(result)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nlangchain_core.messages.ai.AIMessage\n```\n:::\n:::\n\n\n::: {#5aed2300 .cell execution_count=3}\n``` {.python .cell-code}\nprint(result)\nprint(result.response_metadata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncontent='For the best chance to see Orcas in the US, the San Juan Islands of Washington State are widely considered the premier location. These islands, part of the Salish Sea, are home to a large resident pod of orcas that primarily feed on salmon. However, you can also spot Transient (Biggâ€™s) orcas, which hunt marine mammals like seals and sea lions. The summer months (June-September) offer the highest probability of seeing resident orcas, while winter months offer a chance to witness the more elusive Transient orcas. Whale watching tours operate daily from towns like Friday Harbor and Anacortes, offering expert guides and increased chances of a sighting.' additional_kwargs={} response_metadata={'model': 'gemma3:4b', 'created_at': '2025-06-03T23:08:15.261786Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15642801375, 'load_duration': 109076167, 'prompt_eval_count': 73, 'prompt_eval_duration': 1926795958, 'eval_count': 136, 'eval_duration': 13603448792, 'model_name': 'gemma3:4b'} id='run-9b665ac1-cd2b-4060-9866-057814d6ba8d-0' usage_metadata={'input_tokens': 73, 'output_tokens': 136, 'total_tokens': 209}\n{'model': 'gemma3:4b', 'created_at': '2025-06-03T23:08:15.261786Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15642801375, 'load_duration': 109076167, 'prompt_eval_count': 73, 'prompt_eval_duration': 1926795958, 'eval_count': 136, 'eval_duration': 13603448792, 'model_name': 'gemma3:4b'}\n```\n:::\n:::\n\n\n# Using messages as graph state\n\nWith the above foundations in place, we can now use `messages` in our graph state.\n\n-   Let's define our state `MessageState`.\n-   It's defined as a `TypedDict` with a single key:`messages`\n-   `messages` is simply a list of type `AnyMessage`, meaning it's a list of messages.\n\n::: {#11ef3343 .cell execution_count=4}\n``` {.python .cell-code}\nfrom typing import TypedDict\nfrom langchain_core.messages import AnyMessage\n\nclass MessageState(TypedDict):\n    messages: list[AnyMessage]\n```\n:::\n\n\n# Reducers\n\nReducers are functions that allow us to specify how to handle state updates as we move through a graph.\n\nNow, as the graph runs, we do not want to overwrite messages, rather append our messages to our `messages` state key. The above problem motivates the idea of a reducer function.\n\n-   Reducer functions allow us to specify how state updates are performed.\n-   if no reducer function is explicitly specified, then it is assumed that all updates to that key should override it.\n-   Since we want to append messages, we can use a pre-built `add_messages` reducer!\n-   The above ensures that state updates we send to our graph are appended to the existing list of messages.\n-   We annotate our key with a reducer function as metadata using `Annotated` from `typing`.\n\n::: {#53ebc958 .cell execution_count=5}\n``` {.python .cell-code}\nfrom typing import Annotated\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\n\nclass MessageState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n```\n:::\n\n\nSince the above operation is so common, langgraph provides a pre-built `MessagesState` This state has the following: - A pre-built single `messages` key - which is a list of `AnyMessage` objects that uses the `add_messages` reducer.\n\n::: {#f5366e74 .cell execution_count=6}\n``` {.python .cell-code}\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    # Add any other keys you need besides messages, which is pre-built\n    pass\n```\n:::\n\n\nThe classes `State` and `MessageState` work equivalently.\n\nLet's look at how add_messages function works in isolation.\n\n::: {#08bc13e6 .cell execution_count=7}\n``` {.python .cell-code}\ninitial_messages = [AIMessage(content = \"hello, how can I assist you?\"),\nHumanMessage(content = \"I'm looking for information on marine biology\")]\n\n# new message to add\nnew_message = AIMessage(content = \"Sure, I can help with that. What specifically are you interested in?\")\n\n#\nadd_messages(initial_messages, new_message)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n[AIMessage(content='hello, how can I assist you?', additional_kwargs={}, response_metadata={}, id='aafeff24-efa6-48a6-a72f-92d19baacf41'),\n HumanMessage(content=\"I'm looking for information on marine biology\", additional_kwargs={}, response_metadata={}, id='d7448e16-3f60-4a01-9e5b-f33411345148'),\n AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, id='2757efea-ee44-46f4-900e-2d0f680693fa')]\n```\n:::\n:::\n\n\n# Make a graph\n\n::: {#3a4b1bc1 .cell execution_count=8}\n``` {.python .cell-code}\nfrom IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\n# Define a state class\nclass State(MessagesState):\n    # Add any additional keys apart from messages\n    pass\n\n# Define a node\ndef calling_an_llm(state: State):\n    return {\"messages\": [gemma3.invoke(state[\"messages\"])]}\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"llm_call\", calling_an_llm)\nbuilder.add_edge(START, \"llm_call\")\nbuilder.add_edge(\"llm_call\", END)\ngraph = builder.compile()\n\n# View the graph\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n::: {.cell-output .cell-output-display}\n![](06_using_local_chat_models_in_langgraph_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\n# Run the graph\n\n::: {#5c838ec7 .cell execution_count=9}\n``` {.python .cell-code}\nresult = graph.invoke({\"messages\": HumanMessage(content = \"hello, how are you today\")})\nresult\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n{'messages': [HumanMessage(content='hello, how are you today', additional_kwargs={}, response_metadata={}, id='8cac55d8-4004-4383-b4df-12fc4384efaa'),\n  AIMessage(content='Iâ€™m doing great, thanks for asking! As a large language model, I donâ€™t really *feel* in the way humans do, but my systems are running smoothly and Iâ€™m ready to chat. ðŸ˜Š \\n\\nHow about you? How is *your* day going so far? \\n\\nDo you want to talk about something specific, or just have a general conversation?', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-06-03T23:08:23.517882Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8060671291, 'load_duration': 63764250, 'prompt_eval_count': 15, 'prompt_eval_duration': 233351084, 'eval_count': 80, 'eval_duration': 7762857791, 'model_name': 'gemma3:4b'}, id='run-ea93a7c2-2ecf-4f0e-b995-890a570137b8-0', usage_metadata={'input_tokens': 15, 'output_tokens': 80, 'total_tokens': 95})]}\n```\n:::\n:::\n\n\n::: {#76b8e86b .cell execution_count=10}\n``` {.python .cell-code}\nresult = graph.invoke({\"messages\": HumanMessage(content = \"tell me about marine biology in 2 sentences.\")})\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'messages': [HumanMessage(content='tell me about marine biology in 2 sentences.', additional_kwargs={}, response_metadata={}, id='7c62138b-ed49-4415-a47a-b494d14e446e'), AIMessage(content='Marine biology is the scientific study of life in the ocean, encompassing everything from microscopic plankton to massive whales and the complex ecosystems they inhabit. It explores the diverse adaptations, behaviors, and interactions of marine organisms and their environment, crucial for understanding and protecting our oceans.', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-06-03T23:08:29.020415Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5486549750, 'load_duration': 57485916, 'prompt_eval_count': 19, 'prompt_eval_duration': 293024833, 'eval_count': 54, 'eval_duration': 5135490167, 'model_name': 'gemma3:4b'}, id='run-f144c8cb-173e-412a-a99b-f1930eb02664-0', usage_metadata={'input_tokens': 19, 'output_tokens': 54, 'total_tokens': 73})]}\n```\n:::\n:::\n\n\n::: {#362c19e1 .cell execution_count=11}\n``` {.python .cell-code}\nresult[\"messages\"][-1].content\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n'Marine biology is the scientific study of life in the ocean, encompassing everything from microscopic plankton to massive whales and the complex ecosystems they inhabit. It explores the diverse adaptations, behaviors, and interactions of marine organisms and their environment, crucial for understanding and protecting our oceans.'\n```\n:::\n:::\n\n\n",
    "supporting": [
      "06_using_local_chat_models_in_langgraph_files"
    ],
    "filters": [],
    "includes": {}
  }
}