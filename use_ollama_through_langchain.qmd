---
title: "Basics of Langchain and Ollama - Call a local chat model using LangChain"
author: "Shefali Lathwal"
date: "2025-05-30"
date-modified: last-modified
editor: visual
toc: true
format: html
jupyter: python_langchain
---

# Connect to an Ollama Model

Use ollama chat interface with Langchain

ChatOllama is langchain's abstraction to the ollama API endpoint.

```{python}
from langchain_ollama import ChatOllama

# Define a model to use. The model needs to be present locally. The parameter, temperature, sets the creativity of the model, with 0 being least creative and 1 being the most creative.
gemma3 = ChatOllama(model = "gemma3:4b", temperature = 0)
```

# Send a message to a model and get a response

We use two main methods to interact with Chat Models

-   stream - returns partial responses as they are generated. it's best used in chat interfaces where it's useful for users to see answer as it is being generated.
-   invoke - returns the entire response after the model finishes generating. Invoke will be used in agentic workflows where we will use the entire response before proceeding to the next step.

For the rest of the section, we will focus on the invoke method.

Now to get a response back from a model, we first need to pass a text, called prompt, to the model.

We can do the following: - Pass a string directly - pass a list of messages containing dictionaries - pass messages using the class ChatPromptTemplate - pass a list of messages containing different messsages objects

## Directly pass a prompt to the model

```{python}
from IPython.display import Markdown
prompt = "Tell me something about Canada in two sentences."
response = gemma3.invoke(prompt)
Markdown(response.content)
```

## Use a message list with a dictionary containing a role and content fields

Chat models take this prompt as a message. Messages have a:

-   role (that describes who is saying the message)
-   a content property

Just like we saw in the ollama python package, we can get a response back by defining a message as shown.

```{python}
prompt = "Tell me something about Canada in two sentences."
messages = [{"role": "user", "content": prompt}]

response = gemma3.invoke(input = messages)
response
```

If we look at the output above, we see that the model returns an object called `AIMessage`, with the actual message present in the `response.content` field.

## Re-use a prompt by defining a template

We can define a user prompt that takes variables as input and keep re-using the prompt. We need this capability because of the following reasons: 1. As we build more complex workflows, prompts can get long and detailed and we do not want to type them repeatedly. Thereofre, we can create templates and re-use them with minor changes. 2. LangChain provides prompts for some common tasks such as Q&A, summarization, connecting SQL databases, connecting to APIs, so we can use built-in prompt templates from langchain. 3. LangChain support output parsing. You can ask outputs to be in specific formats with tags such as "Thought", "Action", "Observation" and prompt templates are a useful abstraction that allow us to extract these keywords.

-   We define a special object called ChatPromptTemplate

```{python}
from langchain.prompts import ChatPromptTemplate

# Do not use the f-string in the prompt since the variables will be defined later.
template_string = "Tell me about {country} in {num} sentences."
prompt_template = ChatPromptTemplate.from_template(template_string)
prompt_template
```

Extract the actual prompt from the template as follows:

```{python}
prompt_template.messages[0].prompt, prompt_template.messages[0].prompt.input_variables 
```

We can see that the prompt has two input variales.

Create a message from the template using some variable values and pass it to a model

```{python}

country = "France"
num_sentences = "two"

# This code inserts the variables into the ChatPromptTemplate object
message = prompt_template.format_messages(
    country = country,
    num = num_sentences
)
print("User message:", message)
response = gemma3.invoke(input = message)
print("Model response:", response)
Markdown(response.content)
```

## Use the built-in messages classes in LangChain

We are using a class called HumanMessage

```{python}
from langchain_core.messages import HumanMessage
prompt = "Tell me something about Canada in two sentences."
# Create a Message
msg = HumanMessage(content = prompt, name = "Shefali")

# Make a message list
messages = [msg]

# Invoke the model with a list of messages
# Make sure ollama is running on your system
response = gemma3.invoke(messages)
print(response)
Markdown(response.content)
```

We get an `AIMessage` back.

## Use stream method to call the model

Response is printed as it is being generated.

```{python}
for chunk in gemma3.stream("Tell me about Canada in 4 sentences."):
    print(chunk.content, end="")
```